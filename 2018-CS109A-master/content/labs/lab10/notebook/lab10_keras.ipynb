{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <img style=\"float: left; padding-right: 10px; width: 45px\" src=\"https://raw.githubusercontent.com/Harvard-IACS/2018-CS109A/master/content/styles/iacs.png\"> CS109A Introduction to Data Science"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab 10:  Keras for Artificial Neural Networks\n",
    "\n",
    "**Harvard University**<br/>\n",
    "**Fall 2018**<br/>\n",
    "**Instructors:** Pavlos Protopapas and Kevin Rader<br/>\n",
    "**Lab Instructor:** Eleni Kaxiras<br/>\n",
    "**Authors:** David Sondak, Eleni Kaxiras, and Pavlos Protopapas\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "blockquote { background: #AEDE94; }\n",
       "h1 { \n",
       "    padding-top: 25px;\n",
       "    padding-bottom: 25px;\n",
       "    text-align: left; \n",
       "    padding-left: 10px;\n",
       "    background-color: #DDDDDD; \n",
       "    color: black;\n",
       "}\n",
       "h2 { \n",
       "    padding-top: 10px;\n",
       "    padding-bottom: 10px;\n",
       "    text-align: left; \n",
       "    padding-left: 5px;\n",
       "    background-color: #EEEEEE; \n",
       "    color: black;\n",
       "}\n",
       "\n",
       "div.exercise {\n",
       "\tbackground-color: #ffcccc;\n",
       "\tborder-color: #E9967A; \t\n",
       "\tborder-left: 5px solid #800080; \n",
       "\tpadding: 0.5em;\n",
       "}\n",
       "\n",
       "span.sub-q {\n",
       "\tfont-weight: bold;\n",
       "}\n",
       "div.theme {\n",
       "\tbackground-color: #DDDDDD;\n",
       "\tborder-color: #E9967A; \t\n",
       "\tborder-left: 5px solid #800080; \n",
       "\tpadding: 0.5em;\n",
       "\tfont-size: 18pt;\n",
       "}\n",
       "div.gc { \n",
       "\tbackground-color: #AEDE94;\n",
       "\tborder-color: #E9967A; \t \n",
       "\tborder-left: 5px solid #800080; \n",
       "\tpadding: 0.5em;\n",
       "\tfont-size: 12pt;\n",
       "}\n",
       "p.q1 { \n",
       "    padding-top: 5px;\n",
       "    padding-bottom: 5px;\n",
       "    text-align: left; \n",
       "    padding-left: 5px;\n",
       "    background-color: #EEEEEE; \n",
       "    color: black;\n",
       "}\n",
       "header {\n",
       "   padding-top: 35px;\n",
       "    padding-bottom: 35px;\n",
       "    text-align: left; \n",
       "    padding-left: 10px;\n",
       "    background-color: #DDDDDD; \n",
       "    color: black;\n",
       "}\n",
       "</style>\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## RUN THIS CELL TO GET THE RIGHT FORMATTING \n",
    "import requests\n",
    "from IPython.core.display import HTML\n",
    "styles = requests.get\\\n",
    "    (\"https://raw.githubusercontent.com/Harvard-IACS/2018-CS109A/master/content/styles/cs109.css\").text\n",
    "HTML(styles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['axes.labelsize'] = 14\n",
    "plt.rcParams['xtick.labelsize'] = 12\n",
    "plt.rcParams['ytick.labelsize'] = 12\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Anatomy of an Artificial Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a previous lab we created our own neural network by writing some simple python functions.  We focused on a regression problem where we tried to learn a function. We practiced using the logistic activation function in a network with multiple nodes, but a single or two hidden layers.  Some of the key observations were:\n",
    "* Increasing the number of nodes allows us to represent more complicated functions  \n",
    "* The weights and biases have a very big impact on the solution\n",
    "* Finding the \"correct\" weights and biases is really hard to do manually\n",
    "* There must be a better method for determining the weights and biases automatically\n",
    "\n",
    "We also didn't assess the effects of different activation functions or different network depths. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The 3 parts of an ANN\n",
    "\n",
    "- **Part 1: the input layer** (dimentions are determined from our dataset)\n",
    "- **Part 2: the internal architecture or hidden layers** (the number of layers, the activation functions, the learnable parameters and other hyperparameters)\n",
    "- **Part 3: the output layer** (what we want from the network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A word about .npy files\n",
    "\n",
    "Numpy arrays are faster than plain python lists, as we know. Numpy also offers a file format called .npy, which, when it comes to reading the same data multiple times from disk storage, is a lot faster than reading from a csv file. You can save any list or array into this format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('/tmp/123', np.array([[1, 2, 3], [4, 5, 6]]))\n",
    "hello = np.load('/tmp/123.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hello"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `Keras` Basics ![](figs/keras.png)\n",
    "https://keras.io/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Machine learning computations can be quite demanding. TensorFlow is a framework for representing complicated ML algorithms and executing them in any platform, from a phone to a distributed system using GPUs. Developed by Google Brain, TensorFlow is used very broadly today. \n",
    "\n",
    "**[`keras`](https://keras.io/)**, is a high-level API used for fast prototyping, advanced research, and production. We will use `tf.keras` which is TensorFlow's implementation of the `keras` API.\n",
    "\n",
    "### Models are assemblies of layers\n",
    "\n",
    "The core data structure of Keras is a **model**, a way to organize layers. A model is understood as a sequence or a graph of standalone, fully-configurable modules that can be plugged together with as few restrictions as possible. In particular, neural layers, cost functions, optimizers, initialization schemes, activation functions, regularization schemes are all standalone modules that you can combine to create new models.\n",
    "\n",
    "The simplest type of model is the **Sequential** model, a linear stack of layers. For more complex architectures, one can use the Keras **Functional** API, which allows to build arbitrary graphs of layers.\n",
    "\n",
    "https://keras.io/models/model/\n",
    "\n",
    "Everything you need to know about the Sequential model is here: https://keras.io/models/sequential/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keras Installation\n",
    "\n",
    "If you haven't already, install `Keras` using the instructions found at [https://keras.io/#installation](https://keras.io/#installation)\n",
    "\n",
    "Choose the TensorFlow installation instructions (found at [https://www.tensorflow.org/install/](https://www.tensorflow.org/install/) )."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Approximating a Gaussian using keras\n",
    "Let's try to redo the problem from last week.  Recall that we had a function\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "f\\left(x\\right) = e^{-x^{2}}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "and we wanted to use a neural network to approximate that function.  This week, we will use `keras` to do the true optimization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we import the necessary `keras` modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "print(tf.VERSION)\n",
    "print(tf.keras.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking if our machine has GPUs. Mine does not..\n",
    "with tf.Session() as sess:\n",
    "    devices = sess.list_devices()\n",
    "    print(devices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we get started, we need to create some **data**.  We will generate data points from an underlying function (here the Guassian).  Then we will use the `sklearn` `train_test_split` method to split the dataset into training and testing portions.  Remember that we train a machine learning algorithm on the training set and then assess the algorithm's performance on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "n_samples = 1050 # set the number of samples to take for each dataset\n",
    "test_size = 0.3 # set the proportion of data to hold out for testing\n",
    "\n",
    "# define the function and add noise\n",
    "\n",
    "def f_gauss(x):\n",
    "    return np.exp(-x * x) + np.random.normal(loc=0, scale=.1, size = x.shape[0])\n",
    "\n",
    "X = np.random.permutation(np.linspace(-10, 10, n_samples)) # choose some points from the function\n",
    "Y = f_gauss(X)\n",
    "\n",
    "# create training and testing data from this set of points\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=test_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.scatter(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a keras network\n",
    "\n",
    "Now we will create a neural network model with keras. We're going to use a single layer and just 2 neurons in that layer. We will start with the sigmoid activation function. We also choose a linear output layer since we are doing regression. The loss function is selected to be the **mean squared error (MSE)**. In addition to these choices we must also specify our initial weights as well as the optimization method that will be used to minimize the loss function. The keras interface has many choises as to those hyperparameters.\n",
    "\n",
    "**Part 1:** First we start by defining the number of nodes in a layer and the input dimensions. If we have more than one layer we might need to define a value for the number of nodes (H) for each layer.\n",
    "\n",
    "`H = \n",
    "input_dim =`\n",
    "\n",
    "Then we instantiate the model\n",
    "\n",
    "`model = models.Sequential() `\n",
    "\n",
    "**Part 2:** Then we add the hidden layers. Adding layers and stacking them is done using `.add()`\n",
    "\n",
    "`model.add(layers.Dense(H, input_dim=input_dim,  \n",
    "                activation='sigmoid')) `\n",
    "\n",
    "**An alternative way** \n",
    "\n",
    "`model = Sequential([\n",
    "    Dense(200, input_shape=(X_train.shape[1],), activation='relu'),\n",
    "    Dense(100, activation='relu'),\n",
    "    Dense(50, activation='relu'),\n",
    "    Dense(4, activation='linear')\n",
    "])`\n",
    "\n",
    "**Part 3:** We end with the final layer (output)\n",
    "\n",
    "`model.add(layers.Dense(1, \n",
    "                activation='linear')) `\n",
    "                \n",
    "Our model is not ready yet. We need to configure its learning process with .compile():\n",
    "\n",
    "`model.compile(loss='mean_squared_error', optimizer='sgd')`\n",
    "\n",
    "If you need to, you can further configure your optimizer. A core principle of Keras is to make things reasonably simple, while allowing the user to be fully in control when they need to (the ultimate control being the easy extensibility of the source code)\n",
    "\n",
    "`model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.SGD(lr=0.01))`\n",
    "              \n",
    "Our model is now ready to use. We haven't trained it yet, but we'll do that now using the fit method. Notice that we also need to specify the batch size for the stochastic gradient decent algorithm as well as the number of epochs to run.\n",
    "\n",
    "`model.fit(X_train, Y_train, batch_size=100, epochs=100)#, verbose=1)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"exercise\"><b>Exercise 1:</b> </div>\n",
    "\n",
    "Build a NN with one hidden layer with **2 neurons**. Use the `tanh` activation function. Train the model using the X_train dataset from above (train the model in this case means run `.compile` and `.fit`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/NN_1_layer_2_nodes.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great!  We've trained a model.  Now it's time to explore the results.  Notice the loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Some constants for our plots\n",
    "FIG_SIZE = (10,5)\n",
    "FONT_SIZE = 10\n",
    "LABEL_SIZE = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# use our model to predict in the range we want\n",
    "X_range = np.linspace(-10, 10, 1000)\n",
    "y_pred = model.predict(X_range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the results\n",
    "fig, ax = plt.subplots(1, 1, figsize=FIG_SIZE)\n",
    "ax.scatter(X_train, Y_train, label='Training data')\n",
    "ax.plot(X_range, y_pred, lw=4, color='r', label=f'MLP with one hidden layer and {H} nodes')\n",
    "ax.set_xlabel(r'$X$', fontsize=FONT_SIZE)\n",
    "ax.set_ylabel(r'$Y$', fontsize=FONT_SIZE)\n",
    "ax.set_title(f'NN with {len(model_history.model.layers)-1} layers, {H} nodes in each layer', fontsize=LABEL_SIZE)\n",
    "ax.tick_params(labelsize=LABEL_SIZE)\n",
    "\n",
    "ax.legend(loc=0, fontsize=FONT_SIZE)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"exercise\"><b>Exercise 2:</b></div>\n",
    "\n",
    "Change the number of neurons in the layer. Try changing the activation function to `reLU`.  Can you get better results?  What worked the best?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/NN_1_layer_16_nodes.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is the loss smaller now? You may access the results in a model by its `.history`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2_history.history['loss'][-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again let's use the new model to predict:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use our model to predict in the range we want\n",
    "X_range = np.linspace(-10, 10, 1000)\n",
    "y_pred = model2.predict(X_range)\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=FIG_SIZE)\n",
    "ax.scatter(X_train, Y_train, label='Training data', alpha=0.3)\n",
    "ax.scatter(X_test, Y_test, label='Testing data' , alpha=0.3)\n",
    "ax.plot(X_range, y_pred, lw=4, color='r', label=f'NN with one hidden layer and {H} nodes')\n",
    "ax.set_xlabel(r'$X$', fontsize=FONT_SIZE)\n",
    "ax.set_ylabel(r'$Y$', fontsize=FONT_SIZE)\n",
    "ax.set_title(f'NN with {len(model2_history.model.layers)-1} layers, {H} nodes in each layer', fontsize=LABEL_SIZE)\n",
    "ax.tick_params(labelsize=LABEL_SIZE)\n",
    "\n",
    "ax.legend(loc=0, fontsize=FONT_SIZE)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"exercise\"><b>Exercise 3:</b>\n",
    "</div>\n",
    "\n",
    "Plot the loss function as a function of the epochs. <b>Hint:</b> You can access the loss function values with the command:`model_history.history['loss']`\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# %load solutions/print_history.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How good is the model?  We can compute the $R^{2}$ score to get a sense of the model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# evaluate the training and testing performance of your model \n",
    "# note: you should extract and check both the loss function and your evaluation metric\n",
    "from sklearn.metrics import r2_score as r2\n",
    "\n",
    "train_score = model.evaluate(X_train, Y_train, verbose=1)\n",
    "print('Train loss:', train_score)\n",
    "print('Train R2:', r2(Y_train, model.predict(X_train)))\n",
    "\n",
    "test_score = model.evaluate(X_test, Y_test, verbose=1)\n",
    "print('Test loss:', test_score)\n",
    "print('Test R2:', r2(Y_test, model.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"exercise\"><b>Exercise 4</b> </div>\n",
    "\n",
    "Let's add more layers. Fix the width $H$ and fit a MLP network with <b>multiple</b> hidden layers, each with the same width. Start with logistic or hyperbolic-tan activation functions for the hidden nodes and linear activation for the output. Experiment with the number of layers and observe the effect of this on the quality of the fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/NN_10_layers_100_nodes.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# configure the model\n",
    "model3.compile(loss='mean_squared_error', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the model - INTENSIVE\n",
    "model3_history = model3.fit(X_train, Y_train, batch_size=256, epochs=1500, verbose=1, \\\n",
    "                            shuffle = True, validation_split=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(model3_history.model.layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use our model to predict in the range we want\n",
    "num_epochs = f'{len(model2_history.epoch)}'\n",
    "\n",
    "X_range = np.linspace(-10, 10, 500)\n",
    "y_pred = model3.predict(X_range)\n",
    "\n",
    "fig, ax = plt.subplots(1,1, figsize=(10,6))\n",
    "ax.scatter(X_train, Y_train, label='Training data')\n",
    "ax.plot(X_range, y_pred, lw=4, color='r', label=f'NN ({num_epochs} epochs)')\n",
    "\n",
    "ax.set_xlabel(r'$X$', fontsize=20)\n",
    "ax.set_ylabel(r'$Y$', fontsize=20)\n",
    "ax.set_title(f'NN with {len(model3_history.model.layers)} layers, {H} nodes in each layer', fontsize=LABEL_SIZE)\n",
    "ax.tick_params(labelsize=20)\n",
    "\n",
    "ax.legend(loc=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(10,6))\n",
    "ax.plot(np.sqrt(model2_history.history['loss']), 'r')\n",
    "ax.plot(np.sqrt(model2_history.history['val_loss']), 'b' ,label='Val')\n",
    "ax.set_xlabel(r'Epoch', fontsize=20)\n",
    "ax.set_ylabel(r'Loss', fontsize=20)\n",
    "ax.legend()\n",
    "ax.tick_params(labelsize=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the training and testing performance of your model \n",
    "# note: you should extract and check both the loss function and your evaluation metric\n",
    "score = model2.evaluate(X_train, Y_train, verbose=1)\n",
    "print('Train loss:', score)\n",
    "print('Train R2:', r2(Y_train, model2.predict(X_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = model2.evaluate(X_test, Y_test, verbose=1)\n",
    "print('Test loss:', score)\n",
    "print('Test R2:', r2(Y_test, model2.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We got a better score this time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overfitting the model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"exercise\"><b>Exercise 5</b> </div>\n",
    "\n",
    "Usually we want to avoid overfitting of the data to our model. But here we want to achive overfitting! So we can regularize! There are a few reasons why a model overfits. One is the lack of data. So we will try to overfit by reducing the data. Try that with model3 and see if it overfits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Having very few points in our data\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_samples = 50 # set the number of samples to take for each dataset\n",
    "test_size = 0.3 # set the proportion of data to hold out for testing\n",
    "\n",
    "# define the function and add noise\n",
    "\n",
    "def f_gauss(x):\n",
    "    return np.exp(-x * x) + np.random.normal(loc=0, scale=.1, size = x.shape[0])\n",
    "\n",
    "X = np.random.permutation(np.linspace(-10, 10, n_samples)) # choose some points from the function\n",
    "Y = f_gauss(X)\n",
    "\n",
    "# create training and testing data from this set of points\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=test_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.scatter(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# number of hidden nodes\n",
    "H =  100\n",
    "# input dimension\n",
    "input_dim = 1\n",
    "\n",
    "# create sequential multi-layer perceptron\n",
    "model4 = models.Sequential()\n",
    "# layer 0\n",
    "model4.add(layers.Dense(H, input_dim=input_dim,  \n",
    "                activation='tanh')) \n",
    "# layer 1\n",
    "model4.add(layers.Dense(H,\n",
    "                activation='tanh')) \n",
    "# layer 2\n",
    "model4.add(layers.Dense(H,\n",
    "                activation='tanh')) \n",
    "# layer 3\n",
    "model4.add(layers.Dense(H,  \n",
    "                activation='tanh')) \n",
    "# layer 4\n",
    "model4.add(layers.Dense(H,  \n",
    "                activation='tanh')) \n",
    "# layer 5\n",
    "model4.add(layers.Dense(H,  \n",
    "                activation='tanh')) \n",
    "# layer 6\n",
    "model4.add(layers.Dense(H,  \n",
    "                activation='tanh'))\n",
    "# layer 7\n",
    "model4.add(layers.Dense(H,  \n",
    "                activation='tanh'))\n",
    "# layer 8\n",
    "model4.add(layers.Dense(H,  \n",
    "                activation='tanh'))\n",
    "# layer 9\n",
    "model4.add(layers.Dense(H,  \n",
    "                activation='tanh'))\n",
    "# layer 10 - output\n",
    "model4.add(layers.Dense(1, \n",
    "                activation='linear')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# configure the model\n",
    "model4.compile(loss='mean_squared_error', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the model - INTENSIVE\n",
    "model4_history = model4.fit(X_train, Y_train, batch_size=256, epochs=1500, verbose=1, \\\n",
    "                            shuffle = True, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use our model to predict in the range we want\n",
    "num_epochs = f'{len(model4_history.epoch)}'\n",
    "\n",
    "X_range = np.linspace(-10, 10, 500)\n",
    "y_pred = model4.predict(X_range)\n",
    "\n",
    "fig, ax = plt.subplots(1,1, figsize=(10,6))\n",
    "ax.scatter(X_train, Y_train, label='Training data')\n",
    "ax.plot(X_range, y_pred, lw=4, color='r', label=f'NN ({num_epochs} epochs)')\n",
    "\n",
    "ax.set_xlabel(r'$X$', fontsize=20)\n",
    "ax.set_ylabel(r'$Y$', fontsize=20)\n",
    "ax.set_title(f'NN with {len(model4_history.model.layers)} layers, {H} nodes in each layer', fontsize=LABEL_SIZE)\n",
    "ax.tick_params(labelsize=20)\n",
    "\n",
    "ax.legend(loc=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(10,6))\n",
    "ax.plot(np.sqrt(model4_history.history['loss']), 'r')\n",
    "ax.plot(np.sqrt(model4_history.history['val_loss']), 'b' ,label='Val')\n",
    "ax.set_xlabel(r'Epoch', fontsize=20)\n",
    "ax.set_ylabel(r'Loss', fontsize=20)\n",
    "ax.legend()\n",
    "ax.tick_params(labelsize=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularization\n",
    "\n",
    "Let's try adding a regularizer in our model: `kernel_regularizer=regularizers.l2(l2)`. Also let's create a function that takes the number of layers and the l2 value as the input and creates the model.\n",
    "\n",
    "Usage: `def create_dense([10, 20], l2=0.01)` will create a model with two hidden layers of 10 and 20 nodes each, l2=0.01 regularization and num_classes output nodes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import regularizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "H =  100  # number of hidden nodes\n",
    "input_dim = 1\n",
    "\n",
    "model5 = models.Sequential()\n",
    "\n",
    "# Input layer of the neural network with ReLU activation function and L2 regularization\n",
    "model5.add(layers.Dense(H, input_dim=input_dim,  \n",
    "                activation='tanh', \n",
    "                kernel_regularizer=regularizers.l2(0.01)))\n",
    "           \n",
    "# hidden layers\n",
    "model5.add(layers.Dense(H,   \n",
    "                activation='tanh', \n",
    "                kernel_regularizer=regularizers.l2(0.01)))\n",
    "model5.add(layers.Dense(H,   \n",
    "                activation='tanh', \n",
    "                kernel_regularizer=regularizers.l2(0.01)))\n",
    "model5.add(layers.Dense(H,   \n",
    "                activation='tanh', \n",
    "                kernel_regularizer=regularizers.l2(0.01)))\n",
    "model5.add(layers.Dense(H,   \n",
    "                activation='tanh', \n",
    "                kernel_regularizer=regularizers.l2(0.01)))\n",
    "model5.add(layers.Dense(H,   \n",
    "                activation='tanh', \n",
    "                kernel_regularizer=regularizers.l2(0.01)))\n",
    "model5.add(layers.Dense(H,   \n",
    "                activation='tanh', \n",
    "                kernel_regularizer=regularizers.l2(0.01)))\n",
    "# output layer\n",
    "model5.add(layers.Dense(1, \n",
    "                activation='linear')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# configure the model\n",
    "model5.compile(loss='mean_squared_error', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the model - INTENSIVE\n",
    "model5_history = model5.fit(X_train, Y_train, batch_size=256, epochs=1500, verbose=1, \\\n",
    "                            shuffle = True, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use our model to predict in the range we want\n",
    "num_epochs = f'{len(model5_history.epoch)}'\n",
    "\n",
    "X_range = np.linspace(-10, 10, 500)\n",
    "y_pred = model5.predict(X_range)\n",
    "\n",
    "fig, ax = plt.subplots(1,1, figsize=(10,6))\n",
    "ax.scatter(X_train, Y_train, label='Training data')\n",
    "ax.plot(X_range, y_pred, lw=4, color='r', label=f'NN ({num_epochs} epochs)')\n",
    "\n",
    "ax.set_xlabel(r'$X$', fontsize=20)\n",
    "ax.set_ylabel(r'$Y$', fontsize=20)\n",
    "ax.set_title(f'NN with {len(model5_history.model.layers)} layers, {H} nodes in each layer', fontsize=LABEL_SIZE)\n",
    "ax.tick_params(labelsize=20)\n",
    "\n",
    "ax.legend(loc=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That seems very good. Let's see the $R^2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score as r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = model5.evaluate(X_test, Y_test, verbose=1)\n",
    "print('Test loss:', score)\n",
    "print('Test R2:', r2(Y_test, model5.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(10,6))\n",
    "ax.plot(np.sqrt(model5_history.history['loss']), 'r')\n",
    "ax.plot(np.sqrt(model5_history.history['val_loss']), 'b' ,label='Val')\n",
    "ax.set_xlabel(r'Epoch', fontsize=20)\n",
    "ax.set_ylabel(r'Loss', fontsize=20)\n",
    "ax.legend()\n",
    "ax.tick_params(labelsize=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
